{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# Resources Used (as of 06/25/2020)\n",
    "\n",
    "# NLP Resources\n",
    "#  - https://nlp.stanford.edu/IR-book/information-retrieval-book.html\n",
    "#  - https://nlp.stanford.edu/\n",
    "#  - https://www.nltk.org/\n",
    "#  - https://www.nltk.org/book/ch05.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import nltk, re, pandas as pd, numpy as np, string, time, gc as gc, warnings\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns, matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "# Other\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_validate, ShuffleSplit, cross_val_score\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "# Import all the classifiers\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "from sklearn.linear_model import RidgeClassifier, LogisticRegression, SGDClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB, ComplementNB, MultinomialNB, BernoulliNB, CategoricalNB\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other settings\n",
    "gc.enable()\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful helper functions\n",
    "\n",
    "# Function to plot the distributions of each word in the df\n",
    "def plot_df_distributions(df, name):\n",
    "    # Loop through the df\n",
    "    for col in df.columns:        \n",
    "        # Plot the distribution\n",
    "        sns.histplot(data=df[col].to_numpy(), bins=3, \n",
    "                     kde=True).set(title=\"{} Data | {} Distribution\".format(name, str(col)))\n",
    "        # Show the figure\n",
    "        plt.show()\n",
    "               \n",
    "# This function removes numbers from an array\n",
    "def remove_nums(arr): \n",
    "    # Declare a regular expression\n",
    "    pattern = '[0-9]'  \n",
    "    # Remove the pattern, which is a number\n",
    "    arr = [re.sub(pattern, '', i) for i in arr]    \n",
    "    # Return the array with numbers removed\n",
    "    return arr\n",
    "\n",
    "# This function cleans the passed in paragraph and parses it\n",
    "def get_words(para, stem):   \n",
    "    # Create a set of stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # Split it into lower case and remove punctuation\n",
    "    no_punctuation = [nopunc.translate(str.maketrans('', '', string.punctuation)) for nopunc in para.lower().split()]\n",
    "    # Remove integers\n",
    "    no_integers = remove_nums(no_punctuation)\n",
    "    # Remove stop words\n",
    "    dirty_tokens = [data for data in no_integers if data not in stop_words]\n",
    "    # Ensure it is not empty\n",
    "    tokens = [data for data in dirty_tokens if data.strip()]\n",
    "    # Ensure there is more than 1 character to make up the word\n",
    "    tokens = [data for data in tokens if len(data) > 1]\n",
    "       \n",
    "    if stem == True:\n",
    "        # Perform stemming\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
    "        return stemmed_tokens\n",
    "    \n",
    "    else:\n",
    "        # Return the tokens\n",
    "        return tokens \n",
    "    \n",
    "# Function to build 3 frequency representations of the data\n",
    "# TFIDF, Boolean, and TF\n",
    "def build_dataframes(raw, min_df=None, max_df=None):\n",
    "    # Capture the classes\n",
    "    classes = raw['sentiment']\n",
    "    # Capture the reviews as a Series\n",
    "    s = pd.Series(raw['review'])\n",
    "    # Create a total corpus\n",
    "    corpus = s.apply(lambda s: ' '.join(get_words(s, True)))\n",
    "    \n",
    "    ## TODO:\n",
    "    # Could add verbose?\n",
    "    # Probably could put some error handling for min/max df values here\n",
    "    \n",
    "    # Create vectorizers to parse the data\n",
    "    trimmed_boolean_vectorizer = CountVectorizer(strip_accents='unicode',\n",
    "                                             min_df=min_df, max_df=max_df, \n",
    "                                             binary=True)\n",
    "    trimmed_tfidf_vectorizer = TfidfVectorizer(strip_accents='unicode', min_df=min_df, max_df=max_df)\n",
    "    trimmed_count_vectorizer = CountVectorizer(strip_accents='unicode', min_df=min_df, max_df=max_df)\n",
    "    \n",
    "    # Create transformers and fit the corpus\n",
    "    trimmed_tfidf = trimmed_tfidf_vectorizer.fit_transform(corpus)\n",
    "    trimmed_count = trimmed_count_vectorizer.fit_transform(corpus)\n",
    "    trimmed_boolean = trimmed_boolean_vectorizer.fit_transform(corpus)\n",
    "    \n",
    "    # Free up memory\n",
    "    gc.collect()\n",
    "    \n",
    "    # Create dataframes of the results\n",
    "    trimmed_boolean_df = pd.DataFrame(data = trimmed_boolean.todense(), columns = trimmed_boolean_vectorizer.get_feature_names())\n",
    "    trimmed_tfidf_df = pd.DataFrame(data = trimmed_tfidf.todense(), columns = trimmed_tfidf_vectorizer.get_feature_names())\n",
    "    trimmed_count_df = pd.DataFrame(data = trimmed_count.todense(), columns = trimmed_count_vectorizer.get_feature_names())\n",
    "\n",
    "    # Add the classification column to the dataframes\n",
    "    for df in [trimmed_boolean_df, trimmed_tfidf_df, trimmed_count_df]:\n",
    "        df['classification'] = classes\n",
    "        \n",
    "    # Return the three dataframes\n",
    "    return trimmed_boolean_df, trimmed_tfidf_df, trimmed_count_df\n",
    "\n",
    "# Take in the actual classes and the predicted classes and create a report\n",
    "def score_model(actuals, preds):\n",
    "    # Convert to numpy\n",
    "    try:\n",
    "        actuals=actuals.to_numpy()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Record balanced accuracy, accuracy, and sklearn's classification report\n",
    "    bal_acc = balanced_accuracy_score(actuals, preds)\n",
    "    acc = accuracy_score(actuals, preds)\n",
    "    rpt = classification_report(actuals, preds)    \n",
    "    \n",
    "    # Create a pretty print string\n",
    "    print_str = \"\"\"\n",
    "    \n",
    "    Samples: {}\n",
    "    Accuracy: {}\n",
    "    Balanced Accuracy: {}\n",
    "    \n",
    "    Report: {}\n",
    "    \"\"\".format(len(preds), acc, bal_acc, rpt)\n",
    "    \n",
    "    return print_str\n",
    "\n",
    "def run_classification(data, names, classifiers):\n",
    "    # Data: List of dataframes on which to test the algorithms\n",
    "    # Names: Names of their corresponding dataframes\n",
    "    # Classifiers: ML Classifiers to test\n",
    "    ## TODO: Add some smarts to handle input types, lengths, etc. For now, just assume\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        # Assign variables\n",
    "        df = data[i]\n",
    "        classes = df['classification']\n",
    "        df.drop('classification', inplace=True, axis=1)\n",
    "        df_name = names[i]\n",
    "                \n",
    "        # Split into training and testing\n",
    "        X_train, X_test, y_train, y_test = train_test_split(df, classes, stratify=classes, test_size=0.2,\n",
    "                                                           random_state=8)\n",
    "        \n",
    "        for clf in classifiers:\n",
    "            # Get the classifier's name\n",
    "            clf_name = clf.__class__.__name__            \n",
    "            # Fit it (turn to numpy for speed)\n",
    "            train_start = time.time()\n",
    "            clf.fit(X_train.to_numpy(), y_train.to_numpy())\n",
    "            train_stop = time.time()\n",
    "        \n",
    "                    \n",
    "            # Get results\n",
    "            res = score_model(y_test.to_numpy(), X_test.to_numpy())\n",
    "            \n",
    "            # Print results\n",
    "            print(\"REPORT FOR {}\".format(clf_name))\n",
    "            print(\"Trained {} in {}\".format(clf_name, train_stop-train_start))\n",
    "            print(res)\n",
    "            print(\"-----------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities\n",
    "df = pd.read_csv('IMDB Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the DF\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any duplicate values\n",
    "df.drop_duplicates(keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the dataframe again\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show class imbalance (slight)\n",
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a wordcloud for the entire dataset\n",
    "\n",
    "# Create a total corpus\n",
    "corpus = ' '.join(s for s in df['review'])\n",
    "\n",
    "# Generate word cloud\n",
    "wordcloud = WordCloud(width = 3000, height = 2000, random_state=1, background_color='salmon', \n",
    "                      colormap='Pastel1', collocations=False, stopwords = STOPWORDS).generate(corpus)\n",
    "\n",
    "# Plot\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a wordcloud for positive sentiments\n",
    "\n",
    "pos_df = df.loc[df['sentiment'] == 'positive']\n",
    "\n",
    "# Create a total corpus\n",
    "corpus_pos = ' '.join(s for s in pos_df['review'])\n",
    "\n",
    "# Generate word cloud\n",
    "wordcloud = WordCloud(width = 3000, height = 2000, random_state=1, background_color='salmon', \n",
    "                      colormap='Pastel1', collocations=False, stopwords = STOPWORDS).generate(corpus_pos)\n",
    "\n",
    "# Plot\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a wordcloud for positive sentiments\n",
    "\n",
    "neg_df = df.loc[df['sentiment'] == 'negative']\n",
    "\n",
    "# Create a total corpus\n",
    "corpus_pos = ' '.join(s for s in neg_df['review'])\n",
    "\n",
    "# Generate word cloud\n",
    "wordcloud = WordCloud(width = 3000, height = 2000, random_state=1, background_color='salmon', \n",
    "                      colormap='Pastel1', collocations=False, stopwords = STOPWORDS).generate(corpus_pos)\n",
    "\n",
    "# Plot\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boolean, tfidf, freq = build_dataframes(df, min_df=100, max_df=df.shape[0]*0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distributions of boolean dataframe\n",
    "plot_df_distributions(boolean, \"Boolean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distributions of tfidf dataframe\n",
    "plot_df_distributions(tfidf, \"tfidf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distributions of frequency dataframe\n",
    "plot_df_distributions(tfidf, \"frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of classifiers\n",
    "classifiers = [RandomForestClassifier(), AdaBoostClassifier(), ExtraTreesClassifier(),\n",
    "              DecisionTreeClassifier(), ExtraTreeClassifier(), RidgeClassifier(),\n",
    "              LogisticRegression(), SGDClassifier(), MLPClassifier(), KNeighborsClassifier(),\n",
    "              SVC(), GaussianNB(), ComplementNB(), MultinomialNB(), BernoulliNB(),\n",
    "              CategoricalNB(), XGBClassifier(), LGBMClassifier()]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
