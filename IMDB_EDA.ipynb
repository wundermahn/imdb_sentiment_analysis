{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# Resources Used (as of 01/18/2021)\n",
    "\n",
    "# NLP Resources\n",
    "#  - https://nlp.stanford.edu/IR-book/information-retrieval-book.html\n",
    "#  - https://nlp.stanford.edu/\n",
    "#  - https://www.nltk.org/\n",
    "#  - https://www.nltk.org/book/ch05.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import nltk, re, pandas as pd, numpy as np, string, time, gc as gc, warnings\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns, matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "# Other\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_validate, ShuffleSplit, cross_val_score\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.metrics import balanced_accuracy_score, accuracy_score, classification_report\n",
    "\n",
    "# Import all the classifiers\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "from sklearn.linear_model import RidgeClassifier, LogisticRegression, SGDClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB, ComplementNB, MultinomialNB, BernoulliNB, CategoricalNB\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(para):   \n",
    "    # Declare regex pattern, nltk defined stop words, and a Snowball stemmer\n",
    "    pattern = '[0-9]'  \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    \n",
    "    # Remove punctuation\n",
    "    no_punctuation = [nopunc.translate(str.maketrans('', '', string.punctuation)) for nopunc in para.lower().split()]\n",
    "    # Remove integers\n",
    "    no_integers = [re.sub(pattern, '', i) for i in no_punctuation]\n",
    "    # Remove non stop words\n",
    "    dirty_tokens = [data for data in no_integers if data not in stop_words]\n",
    "    # No empty spaces\n",
    "    tokens = [data for data in dirty_tokens if data.strip()]\n",
    "    # No single character \"words\"\n",
    "    tokens = [data for data in tokens if len(data) > 1]\n",
    "    # Stem the remaining tokens\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
    "    # Return them\n",
    "    return stemmed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other settings\n",
    "gc.enable()\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read ind ata\n",
    "df = pd.read_csv('IMDB Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any duplicate values\n",
    "df.drop_duplicates(keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a wordcloud for the entire dataset\n",
    "\n",
    "# Create a total corpus\n",
    "corpus_all = ' '.join(s for s in df['review'])\n",
    "\n",
    "# Generate word cloud\n",
    "wordcloud = WordCloud(width = 3000, height = 2000, random_state=1, background_color='salmon', \n",
    "                      colormap='Pastel1', collocations=False, stopwords = STOPWORDS).generate(corpus_all)\n",
    "\n",
    "# Plot\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a wordcloud for positive sentiments\n",
    "\n",
    "pos_df = df.loc[df['sentiment'] == 'positive']\n",
    "\n",
    "# Create a total corpus\n",
    "corpus_pos = ' '.join(s for s in pos_df['review'])\n",
    "\n",
    "# Generate word cloud\n",
    "wordcloud = WordCloud(width = 3000, height = 2000, random_state=1, background_color='salmon', \n",
    "                      colormap='Pastel1', collocations=False, stopwords = STOPWORDS).generate(corpus_pos)\n",
    "\n",
    "# Plot\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a wordcloud for negative sentiments\n",
    "\n",
    "neg_df = df.loc[df['sentiment'] == 'negative']\n",
    "\n",
    "# Create a total corpus\n",
    "corpus_pos = ' '.join(s for s in neg_df['review'])\n",
    "\n",
    "# Generate word cloud\n",
    "wordcloud = WordCloud(width = 3000, height = 2000, random_state=1, background_color='salmon', \n",
    "                      colormap='Pastel1', collocations=False, stopwords = STOPWORDS).generate(corpus_pos)\n",
    "\n",
    "# Plot\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the classes (sentiments)\n",
    "classes = df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move reviews into a series\n",
    "s = pd.Series(df['review'])\n",
    "# Create a total corpus\n",
    "corpus = s.apply(lambda s: ' '.join(get_words(s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a frequency representation of the data\n",
    "count_vectorizer = CountVectorizer(strip_accents='unicode', min_df=50, max_df=df.shape[0]*0.99)\n",
    "count = count_vectorizer.fit_transform(corpus)\n",
    "count_df = pd.DataFrame(data=count.todense(), columns=count_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a boolean representaion of the data\n",
    "boolean_vectorizer = CountVectorizer(strip_accents='unicode',\n",
    "                                         min_df=50, max_df=df.shape[0]*0.99, \n",
    "                                         binary=True)\n",
    "boolean = boolean_vectorizer.fit_transform(corpus)\n",
    "boolean_df = pd.DataFrame(data=boolean.todense(), columns=boolean_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tfidf representaion of the data\n",
    "tfidf_vectorizer = TfidfVectorizer(strip_accents='unicode', min_df=50, max_df=df.shape[0]*0.99)\n",
    "tfidf = tfidf_vectorizer.fit_transform(corpus)\n",
    "tfidf_df = pd.DataFrame(data=tfidf.todense(), columns=tfidf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of classifiers\n",
    "classifiers = [GaussianNB(), ComplementNB(), MultinomialNB(), BernoulliNB(),\n",
    "              RandomForestClassifier(), AdaBoostClassifier(), ExtraTreesClassifier(),\n",
    "              DecisionTreeClassifier(), ExtraTreeClassifier(), RidgeClassifier(),\n",
    "              LogisticRegression(), SGDClassifier(), MLPClassifier(), KNeighborsClassifier(),\n",
    "              SVC(), XGBClassifier(), LGBMClassifier()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test Frequency Data\n",
    "df = count_df.copy()\n",
    "\n",
    "# Split into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, classes, stratify=classes, test_size=0.2,\n",
    "                                                   random_state=8)\n",
    "\n",
    "for clf in classifiers:\n",
    "    gc.collect()\n",
    "    # Get the classifier's name\n",
    "    clf_name = clf.__class__.__name__            \n",
    "    # Fit it (turn to numpy for speed)\n",
    "    train_start = time.time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_stop = time.time()\n",
    "\n",
    "\n",
    "    # Get predictions\n",
    "    preds = clf.predict(X_test)\n",
    "    \n",
    "    # Record balanced accuracy, accuracy, and sklearn's classification report\n",
    "    bal_acc = balanced_accuracy_score(y_test, preds)\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    rpt = classification_report(y_test, preds)    \n",
    "    \n",
    "    # Create a pretty print string\n",
    "    print_str = \"\"\"\n",
    "    \n",
    "    Samples: {}\n",
    "    Accuracy: {}\n",
    "    Balanced Accuracy: {}\n",
    "    \n",
    "    Report: {}\n",
    "    \"\"\".format(len(preds), acc, bal_acc, rpt)    \n",
    "    \n",
    "\n",
    "    # Print results\n",
    "    print(\"{} | REPORT FOR {}\".format(\"Frequency\", clf_name))\n",
    "    print(\"Trained {} in {}\".format(clf_name, train_stop-train_start))\n",
    "    print(print_str)\n",
    "    print(\"-----------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test TFIDF Data\n",
    "df = tfidf_df.copy()\n",
    "\n",
    "# Split into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, classes, stratify=classes, test_size=0.2,\n",
    "                                                   random_state=8)\n",
    "\n",
    "for clf in classifiers:\n",
    "    gc.collect()\n",
    "    # Get the classifier's name\n",
    "    clf_name = clf.__class__.__name__            \n",
    "    # Fit it (turn to numpy for speed)\n",
    "    train_start = time.time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_stop = time.time()\n",
    "\n",
    "\n",
    "    # Get predictions\n",
    "    preds = clf.predict(X_test)\n",
    "    \n",
    "    # Record balanced accuracy, accuracy, and sklearn's classification report\n",
    "    bal_acc = balanced_accuracy_score(y_test, preds)\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    rpt = classification_report(y_test, preds)    \n",
    "    \n",
    "    # Create a pretty print string\n",
    "    print_str = \"\"\"\n",
    "    \n",
    "    Samples: {}\n",
    "    Accuracy: {}\n",
    "    Balanced Accuracy: {}\n",
    "    \n",
    "    Report: {}\n",
    "    \"\"\".format(len(preds), acc, bal_acc, rpt)    \n",
    "    \n",
    "\n",
    "    # Print results\n",
    "    print(\"{} | REPORT FOR {}\".format(\"TFIDF\", clf_name))\n",
    "    print(\"Trained {} in {}\".format(clf_name, train_stop-train_start))\n",
    "    print(print_str)\n",
    "    print(\"-----------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test boolean Data\n",
    "df = boolean_df.copy()\n",
    "\n",
    "# Split into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, classes, stratify=classes, test_size=0.2,\n",
    "                                                   random_state=8)\n",
    "\n",
    "for clf in classifiers:\n",
    "    gc.collect()\n",
    "    # Get the classifier's name\n",
    "    clf_name = clf.__class__.__name__            \n",
    "    # Fit it (turn to numpy for speed)\n",
    "    train_start = time.time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_stop = time.time()\n",
    "\n",
    "\n",
    "    # Get predictions\n",
    "    preds = clf.predict(X_test)\n",
    "    \n",
    "    # Record balanced accuracy, accuracy, and sklearn's classification report\n",
    "    bal_acc = balanced_accuracy_score(y_test, preds)\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    rpt = classification_report(y_test, preds)    \n",
    "    \n",
    "    # Create a pretty print string\n",
    "    print_str = \"\"\"\n",
    "    \n",
    "    Samples: {}\n",
    "    Accuracy: {}\n",
    "    Balanced Accuracy: {}\n",
    "    \n",
    "    Report: {}\n",
    "    \"\"\".format(len(preds), acc, bal_acc, rpt)    \n",
    "    \n",
    "\n",
    "    # Print results\n",
    "    print(\"{} | REPORT FOR {}\".format(\"Boolean\", clf_name))\n",
    "    print(\"Trained {} in {}\".format(clf_name, train_stop-train_start))\n",
    "    print(print_str)\n",
    "    print(\"-----------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distributions of boolean data\n",
    "# Loop through the df\n",
    "for col in boolean_df.columns:        \n",
    "    # Plot the distribution\n",
    "    sns.histplot(data=boolean_df[col].to_numpy(), bins=3, \n",
    "                 kde=True).set(title=\"{} Data | {} Distribution\".format(\"Boolean\", str(col)))\n",
    "    # Show the figure\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distributions of frequency data\n",
    "# Loop through the df\n",
    "for col in count_df.columns:        \n",
    "    # Plot the distribution\n",
    "    sns.histplot(data=count_df[col].to_numpy(), bins=3, \n",
    "                 kde=True).set(title=\"{} Data | {} Distribution\".format(\"Frequency\", str(col)))\n",
    "    # Show the figure\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distributions of tfidf data\n",
    "# Loop through the df\n",
    "for col in tfidf_df.columns:        \n",
    "    # Plot the distribution\n",
    "    sns.histplot(data=tfidf_df[col].to_numpy(), bins=3, \n",
    "                 kde=True).set(title=\"{} Data | {} Distribution\".format(\"Frequency\", str(col)))\n",
    "    # Show the figure\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
